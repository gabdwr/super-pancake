name: Token Metrics Hourly Fetch

# Run every hour to collect metrics and apply filters for all discovered tokens
on:
  schedule:
    # Runs every hour at :30 (30 minutes after discovery scraper at :00)
    - cron: '30 * * * *'

  # Allow manual triggering from GitHub Actions UI
  workflow_dispatch:

jobs:
  fetch-metrics:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max (GitHub Actions limit)

    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ðŸ”§ Create .env file from secrets
        run: |
          cat > .env << EOF
          SUPABASE_HOST=${{ secrets.SUPABASE_HOST }}
          SUPABASE_PORT=${{ secrets.SUPABASE_PORT }}
          SUPABASE_USERNAME=${{ secrets.SUPABASE_USERNAME }}
          SUPABASE_PASSWORD=${{ secrets.SUPABASE_PASSWORD }}
          SUPABASE_DBNAME=${{ secrets.SUPABASE_DBNAME }}
          TELEGRAM_BOT_TOKEN=${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID=${{ secrets.TELEGRAM_CHAT_ID }}
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}
          SUPABASE_ANON_KEY=${{ secrets.SUPABASE_ANON_KEY }}
          FILTER_ALLOW_HONEYPOT=${{ secrets.FILTER_ALLOW_HONEYPOT }}
          FILTER_MIN_LP_LOCKED=${{ secrets.FILTER_MIN_LP_LOCKED }}
          FILTER_MIN_CONCENTRATION=${{ secrets.FILTER_MIN_CONCENTRATION }}
          FILTER_MIN_LIQUIDITY_USD=${{ secrets.FILTER_MIN_LIQUIDITY_USD }}
          FILTER_MAX_BUY_TAX=${{ secrets.FILTER_MAX_BUY_TAX }}
          FILTER_MAX_SELL_TAX=${{ secrets.FILTER_MAX_SELL_TAX }}
          FILTER_ALLOW_MINTABLE=${{ secrets.FILTER_ALLOW_MINTABLE }}
          EOF

      - name: ðŸ“Š Fetch token metrics and apply filters
        id: datafetch
        run: |
          python run_datafetch_and_filtration.py
        continue-on-error: true

      - name: ðŸ“ˆ Check results
        run: |
          if [ "${{ steps.datafetch.outcome }}" == "failure" ]; then
            echo "âŒ Data fetch failed"
            exit 1
          else
            echo "âœ… Data fetch completed successfully"
          fi

      - name: ðŸ§¹ Cleanup .env file
        if: always()
        run: |
          rm -f .env

# Workflow summary:
# - Runs every hour at :30 (30 minutes after discovery scraper)
# - Fetches metrics for ALL tokens in discovered_tokens table
# - Applies critical filters and tags tokens as PASS/FAIL
# - Calls DexScreener API + GoPlus API for each token (with rate limiting & retries)
# - Stores time-series snapshots in time_series_data table
# - Updates filter_status in discovered_tokens table
# - Sends Telegram notification with summary
# - Can be manually triggered from GitHub Actions tab
#
# Why hourly?
# - Fast filtering: Tokens filtered within ~30 min of discovery (vs 6 hours before)
# - Fresh data: Catch pumps/dumps early, enable faster decision making
# - 24 snapshots/day = rich time-series for trend analysis
# - API limits: 300 tokens Ã— 24 runs = 7,200 calls/day (within free tier)
#   - DexScreener: 300/min = 18,000/hour (safe)
#   - GoPlus: 60/min = 3,600/hour (safe)
#
# Performance notes:
# - ~167 tokens: ~3-5 minutes (with rate limiting & retries)
# - ~300 tokens: ~5-8 minutes
# - ~1000 tokens: ~15-20 minutes
#
# Timing:
# - Discovery runs: :00 (e.g., 1:00, 2:00, 3:00...)
# - Datafetch runs: :30 (e.g., 1:30, 2:30, 3:30...)
# - Max delay: 30 minutes from discovery to filtering
